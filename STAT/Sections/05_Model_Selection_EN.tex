%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Section 5%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Data Reduction/Model Selection} \label{sec:Data.Red}
In a good model, a balance must  be struck between \textbf{predictive ability} and \textbf{simplicity}. In practice, we look for the \textbf{simplest model} that explains the behaviour of the response variable $Y$ in a \textbf{reasonably adequate manner} (a version of \textit{Occam's Razor}). \par If there are $p$ predictor variables $X_1,\ldots,X_p$, then there are $2^{p}$ possible models from which to select the "best", ranging from the \textbf{simple average model} $${y}_{i}=\beta_{0}+\varepsilon_{i}$$ to the \textbf{full model} $${y}_{i}=\beta_{0}+\sum_{j=1}^{p}\beta_{j}x_{i,j}+\varepsilon_{i}.$$

\subsection{Step-Wise Regression}
As the number of predictors $p$ grows, it is not feasible to fit all $2^{p}$ possible models to determine the optimal model. \newl \textbf{Step-wise regression} is an automated model selection procedure that builds a succession of models from which a choice can be made. There are numerous variants -- the particular algorithm we present is called \textbf{forward selection}, for reasons that will shortly become clear (to fix the problem in conceptual space, assume that there are $p=10$ predictor variables).
\begin{enumerate}
    \item \textbf{Selecting the first variable:} fit $p$ simple linear regressions $$y_i=\beta_0 + \beta_jx_{i,j}+\varepsilon_i,\quad j=1,\ldots, p$$ and choose the model with highest $R^{2}$ value. In other words, select the variable $X_j$ that best describes the behaviour of $Y$ \textbf{on its own}. If $X_{5}$ turns out to be that variable, fsay, then the tentative model is $$y_{i}=\beta_{0}+\beta_{5}X_{i,5}+\varepsilon_{i}.$$ If this model is not statistically significant (tested at a predetermined significance level $\alpha$), then the final model selection is $$y_{i}=\beta_{0}+\varepsilon_{i}$$ and the search is complete. Otherwise, proceed to step 2.
    \item \textbf{Selecting the second variable:} fit all two-parameter regression models $$y_i=\beta_0 + \beta_5x_{i,5}+ \beta_jx_{i,j}+\varepsilon_i,\quad j=1,\ldots, p,\quad j\neq 5.$$ Select the model that has the highest value of the test statistic $$t{'}_{k}=\sqrt{\frac{\text{MS}_{\textrm{reg}}(X_{5},X_{k})-\text{MS}_{\textrm{reg}}(X_{5})}{\text{MS}_{\textrm{e}}(X_{5}, X_{k})}}.$$ Say that $k=3$ yields the largest such value. If the associated model's $p-$value is smaller than $\alpha$, then our tentative model is updated to $$y_{i}=\beta_{0}+\beta_{3}X_{i,3}+\beta_{5}X_{i,5}+\varepsilon_{i}$$ and we proceed to step 3. Otherwise, the final model selection is $$y_{i}=\beta_{0}+\beta_{5}X_{i,5}+\varepsilon_{i}$$ and the search is complete.  
    \item \textbf{All subsequent steps:} Repeat step 2 using $$t^{''}_k=\sqrt{\frac{\text{MS}_{\textrm{reg}}(X_{5},X_3,X_k)-\text{MS}_{\textrm{reg}}(X_{5},X_3)}{\text{MS}_{\textrm{e}}(X_{5}, X_3,X_{k})}},$$ and so forth, until no additional term improves the model significantly.
\end{enumerate}
In contrast to forward selection which starts with the simple average model $$y_{i}=\beta_{0}+\varepsilon_{i}$$ and build a nested sequence of increasingly complex models, \textbf{backward elimination} begins with the full model $${y}_{i}=\beta_{0}+\sum_{j=1}^{p}\beta_{j}x_{i,j}+\varepsilon_{i}$$ and keeps removing terms until removal of \textit{any} variable causes a significant loss of its predictive power (calculated using $t^{(\ell)}_{k}$). In general, forward selection and backward elimination will not select the same final model. 
\newpage\noindent
In the \textbf{combined approach}, the process starts from the simple average model as in forward selection, but each time a new variable is added to the tentative model, a backward elimination search is performed to test whether any of the previously added variables are no longer significant, which can prevent \textbf{overfitting} (mistaking noise for a pattern). 
\par The test statistic $t^{(\ell)}_{k}$ is the square root of the ratio of conditional MSR over MSE. In everyday terms, it is testing \textit{whether the addition of $X_{k}$ provides a significant improvement in predictive ability over the current tentative model's}. Other alternative include the \textbf{Akaike Information Criterion} (AIC), the \textbf{Bayesian Information Criteria} (BIC), \textbf{Mallow's $\bm{C_{p}}$ Criterion}, and the \textbf{$\bm{R^2}$ criterion} -- simply pick the model which optimises the desired criterion. \newl Note that step-wise regression is \textbf{flawed} in many ways which we will not explore at the moment; in practice, it has  started being replaced by \textbf{regularisation methods} such as ridge regression and the LASSO.
