Ensemble analysis has been used particularly effectively in the detection of large outliers [\cite{A8}, \cite{A13}, \cite{A14}], in which several subspaces of the data are often searched for outliers, as they are often hidden in \textbf{smaller subspaces}. It is therefore logical to explore the \textbf{lower dimensional subspaces} [\cite{A1}, \cite{A2}]. Such an approach eliminates the additive noise effects of many dimensions and leads to more robust outliers. Such a problem is very difficult to solve effectively. This is due to the fact that the number of possible projections of large dimensional data is exponentially related to the dimensionality of the data. The problem of outlier detection is like finding a needle in a haystack \cite{A14}.
Feature bagging algorithm is presented below
\begin{algorithm}
\SetAlgoLined
j=1\;
\While{As long as the condition is verified }{
  Randomly draw an integer $r$ such that $d/2\leq r\leq d- 1$\;
	Select $r$ dimensions ( variables) from the data randomly to create a $r$ dimension projection\;
	Find LOF score for each point in the projected representation\;
	j=j+1\;
}
\KwResult{Report combined score based on different subspaces}
\caption{FeatureBagging(Data: D)}
\end{algorithm}

\noindent There are many different algorithms in the literature, including the \textbf{HOS} (High-dimensional Outlying Subspaces) \cite{Zhang}; \textbf{SOD} (Subspace Outlier Degree) \cite{Zi} implemented in the \textbf{HighDimOut} package; \textbf{OutRank} (Projected Clustering Ensembles) \cite{M1}; \textbf{OUTRES} (Local Selection of Subspace Projections) \cite{M3}.

In summary, it should be noted that anomaly detection and outlier analysis is still an open field with many challenges. There is no magic method, all methods have strengths. 