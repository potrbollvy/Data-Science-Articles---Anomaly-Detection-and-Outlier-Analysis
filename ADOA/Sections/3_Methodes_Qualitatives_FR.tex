\section{Valeurs aberrantes et données qualitatives}  \label{Section:3}
% \textcolor{red}{Soufiane} 
%
\subsection{D\'efinitions et défis}
%

\subsubsection*{Variables catégoriques}
Une variable catégorique, appartenant à l’échelle nominale  ( Des exemples de ce type sont la couleur, la direction, la langue, etc.). 
La tendance centrale des valeurs d'une variable catégorique est donnée par son mode, alors que des valeurs numériques puissent apparaître correspondant à une variable catégorique, elles représentent chacune un concept distinct et ne peuvent pas être traitées comme des nombres. 
Une variable catégorielle pouvant prendre exactement deux valeurs est appelée variable dichotomique (binaire). Les variables avec plus de deux valeurs possibles sont appelées variables polytomiques. L'analyse de régression sur les variables catégorielles est réalisée par régression logistique multinomiale.



\subsubsection*{Défis avec les données catégoriques}
Lorsqu'on traite des données ayant des attributs catégoriels, on suppose que ces attributs  pourraient facilement être representer en valeurs numériques. Cependant, il existe des exemples d'attributs catégoriels, dans lesquels ces represenration en attributs numériques n'est pas un processus simple. Par conséquent, des méthodes telles que celles basées sur des mesures de distance ou de densité ne sont pas acceptables, nécessitant une modification nécessaire de leurs formulations.

\subsection{Revue de quelques méthodes}


\subsubsection*{Algorithm AVF}

L'algorithme AVF (Attribute Value Frequency) est une approche simple et rapide pour détecter les valeurs aberrantes dans les données catégorielles, ce qui minimise les analyses sur les données, sans avoir besoin de créer ou de chercher parmi différentes combinaisons d’attributs ou d'ensembles d'éléments.  \\

\noindent Il est intuitif que les valeurs aberrantes sont les points qui sont peu fréquents dans l'ensemble de données, et que le point aberrant «idéal» dans un ensemble de données catégoriel est un point dont chaque valeur est extrêmement irrégulière (ou peu fréquente). \\


\noindent La rareté d'une valeur d'attribut peut être mesurée en calculant le nombre de fois où cette valeur est assumée par l'attribut correspondant dans l'ensemble de données. \\

\noindent Supposons qu'il y ait $n$ points dans l'ensemble de données : $x_i$ , $I = 1 \cdots n$ et chaque point de données a m attributs. Nous pouvons écrire que $x_{i} = [x_{i1}, \cdots , x_{il}, \cdots, x_{im}]$, avec $x_{il}$ est la valeur du l-ième attribut de $x_i$. Suivant le raisonnement donné ci-dessus, le score AVF ci-dessous est un bon indicateur pour décider si $x_i$ est une valeur aberrante:

$$AVFscore(x_i) = \frac{1}{m} \sum_{l=1}^{m}f(x_{il})$$


\noindent où $f(x_{il})$ est le nombre de fois où la l-ième valeur d'attribut de $x_i$ apparaît dans l'ensemble de données. Un score AVF inférieur signifie qu'il est plus probable que le point soit une valeur aberrante. Étant donné que $AVFscore(x_i) $ est essentiellement une somme de $m$ nombres positifs, le score AVF est minimal lorsque chacun des termes de la somme est minimisé individuellement. Ainsi, la valeur aberrante «idéale» telle que définie ci-dessus aura le score AVF minimum. Le score minimum sera atteint lorsque chaque valeur du point de données n'apparaît qu'une seule fois.  


\begin{algorithm}
\SetAlgoLined
Étiquetez tous les points de données comme non aberrants \;

Calculez la fréquence de chaque valeur d'attribut, $f(x_{il})$ \;

i = 1 \;

\While{ $i \leq n$ }{
	j = 1
  	\While{$j \leq m$}{
	$AVFscore(x_i) += f(x_{il})$\;
	 $j =  j +1$\; 
	}
	Moyenne( $AVFscore(x_i)$)\;
	$i = i + 1$\;
}

\KwResult{ top $k$ outliers with minimum AVF Score}
\caption{AVF Pseudocode $D$: Données ($n$ points, $m$ attributes)}
\label{alg:AVF}
\end{algorithm}%\newl

\noindent Comme le montre le pseudocode de l'AVF (voir  Algorithm~\ref{alg:AVF}), une fois le score AVF est calculé pour tous les points, les $k$ valeurs aberrantes renvoyées sont les k points avec les scores AVF les plus petits. 

\noindent La complexité de l'AVF est $O(n\times m)$, où $n$ est le nombre de points de données et $m$ est la dimensionnalité de l'ensemble de données.


\subsubsection*{Algorithm Greedy}

Dans cette section, nous présentons un algorithme  "Greedy", not é greedyAlg1, qui est efficace  pour identifier les valeurs aberrantes.
Notre algorithme "greedyAlg1" prend le nombre de valeurs aberrantes souhaitées (supposé être k) en entrée et sélectionne les points comme des valeurs aberrantes d'une manière "Greedy". 
Initialement, l'ensemble des valeurs aberrantes (désigné par "OS") estspécifié comme étant vide et tous les points sont marqués comme non aberrants. 
Ensuite, nous avons besoin de "k" scans sur l'ensemble de données pour sélectionner "k" points comme valeurs aberrantes. 
Dans chaque balayage, pour chaque point étiqueté comme non aberrant, il est temporairement retiré de l'ensemble de données comme valeur aberrante et l'objectif d'entropie est le réévalué.
Un point qui produit "un impact d'entropie maximal", c'est-à-dire la diminution maximale de l'entropie implique la suppression de ce point est sélectionnée comme valeur aberrante dans l'analyse en cours et ajoutée au "OS". L'algorithmese termine lorsque la taille du "OS" atteint $k$.


Pour plus de details sur l'Algorithme voici la l'article source : \url{https://arxiv.org/pdf/cs/0507065.pdf} 




