\newpage\ \newpage\section{Introduction} Isaac Asimov, the prolific American author, once wrote that \begin{quote} The most exciting phrase to hear [...], the one that heralds the most discoveries, is not ``Eureka!'' but ``That's funny...''.\end{quote}
However, anomalous observations are not only harbingers of great scientific discoveries -- unexpected observations can spoil analyses or be indicative of the presence of issues related to data collection or data processing. \par Either way, it becomes imperative for decision-makers and analysts to establish anomaly detection protocols, and to identify strategies to deal with such observations.  
\subsection{Basic Notions and Overview}
\textbf{Outlying observations} are data points which are \textbf{atypical} in comparison to the unit's remaining features (\textit{within-unit}), or in comparison to the measurements for other units (\textit{between-units}), or as part of a collective subset of observations. Outliers are thus observations which are \textbf{dissimilar to other cases} or which contradict \textbf{known dependencies} or rules.\footnote{Outlying observations may be anomalous along any of the individual variables, or in combination.}% (information in this section is taken partly from \cite{DP_OW,DP_A,DP_T,DP_CBK}).
\par Observations could be anomalous in one context, but not in another. Consider, for instance, an adult male who is 6-foot tall. Such a man would fall in the 86th percentile among Canadian males \cite{DP_HPC}, which, while on the tall side, is not unusual; in Bolivia, however, the same man would land in the 99.9th percentile \cite{DP_HPC}, which would mark him as extremely tall and quite dissimilar to the rest of the population.\footnote{Anomaly detection points towards interesting questions for analysts and subject matter experts: in this case, why is there such a large discrepancy in the two populations?}  
\newl
A common mistake that analysts make when dealing with outlying observations is to remove them from the dataset without carefully studying whether they are \textbf{influential data points}, that is, observations whose absence leads to \textbf{markedly different} analysis results.\par When influential observations are identified, remedial measures (such as data transformation strategies) may need to be applied to minimize any undue effect. Note that outliers may be influential, and influential data points may be outliers, but the conditions are neither necessary nor sufficient. 
\subsubsection*{Detecting Anomalies}  Anomalies are by definition \textbf{infrequent}, and typically surrounded by \textbf{uncertainty} due to small (relative) sample sizes, which makes differentiating them from \textbf{noise} or \textbf{data entry errors} difficult. \par It could also be the case that the boundaries between a normal unit and a deviating unit is \textbf{fuzzy}; with the advent of e-shops, a purchase made at 3am (local time) does not necessarily ring alarm bells anymore. \par It is hard enough as it is to try to identify ``honest'' anomalies; when anomalies are associated with \textbf{malicious activities}, they are typically \textbf{disguised} to look like a normal observation, which muddies the picture even more. 
\newl
Numerous methods exist to identify anomalous observations; \textbf{none of them are foolproof} and judgement must be used. Methods that employ graphical aids (such as box-plots, scatterplots, scatterplot matrices, and 2D tours) to identify outliers are particularly easy to implement, but a low-dimensional setting is usually required for ease of interpretability. \par Analytical methods also exist (using Cooke's or Mahalanobis' distances, say), but in general some additional level of analysis must be performed, especially when trying to identify influential points (\textit{cf.} \textbf{leverage}). 
\newl With small datasets, anomaly detection can be conducted on a case-by-case basis, but with large datasets, the temptation to use \textbf{automated detection/removal} is strong -- care must be exercised before the analyst decides to go down that route.\footnote{This stems partly from the fact that that once the ``anomalous'' observations have been removed from the data set, previously ``regular'' observations can become anomalous in turn in the smaller dataset; it is not clear when that runaway train will stop.}\par
In the early stages of anomaly detection, \textbf{simple data analyses} (such as descriptive statistics, 1- and 2-way tables, and  traditional visualisations) may be performed to help identify anomalous observations, or to obtain insights about the data, which could eventually lead to modifications of the analysis plan.
\subsubsection*{Outlier Tests} How are outliers \textit{actually} detected? Most methods come in one of two flavours: \textbf{supervised} and \textbf{unsupervised} (we will discuss those in detail in later sections).\par
Supervised methods use a historical record of \textbf{labeled} (that is to say, previously identified) anomalous observations to build a \textbf{predictive classification or regression model} which estimates the probability that a unit is anomalous; domain expertise is required to tag the data. Since anomalies are typically \textbf{infrequent}, these models often also have to accommodate the \textbf{rare occurrence problem}.\footnote{Supervised models are built to minimize a cost function; in default settings, it is often the case that the mis-classification cost is assumed to be symmetrical, which can lead to technically correct but useless solutions. For instance, the vast majority (99.999+\%) of air passengers emphatically do not bring weapons with them on flights; a model that predicts that no passenger is attempting to smuggle a weapon on board a flight would be 99.999+\% accurate, but it would miss the point completely.}\par Unsupervised methods, on the other hand, use no previously labeled information or data, and try to determine if an observation is an outlying one solely by comparing its behaviour to that of the other observations. \newpage\noindent The following traditional methods and tests of outlier detection fall into this category:\footnote{Note that \textbf{normality} of the underlying data is an assumption for most tests; how robust these tests are against departures from this assumption depends on the situation.}
\begin{itemize}[noitemsep]
\item Perhaps the most commonly-used test is \textbf{Tukey's boxplot test}; for normally distributed data, regular observations typically lie between the \textbf{inner fences} $$Q_1-1.5(Q_3-Q_1) \quad\mbox{and}\quad Q_3+1.5(Q_3-Q_1).$$ \textbf{Suspected outliers} lie between the inner fences and the \textbf{outer fences} 
$$Q_1-1.5(Q_3-Q_1) \quad\mbox{and}\quad Q_3+1.5(Q_3-Q_1).$$
Points beyond the outer fences are identified as \textbf{outliers} ($Q_1$ and $Q_3$ represent the data's   $1^{\textrm{st}}$ and $3^{\textrm{rd}}$ quartile, respectively; see Figure~\ref{fig:boxplot}).
\begin{figure}[t]
\centering
\includegraphics[width=0.30\textwidth]{Images/boxplot.png}
\caption[\small Tukey's boxplot test for outliers]{\small Tukey's boxplot test; suspected outliers are marked by white disks, outliers by black disks.}
\hrule\label{fig:boxplot}
\end{figure}
\afterpage{\FloatBarrier}
\item The \textbf{Grubbs test} is another univariate test, which takes into consideration the number of observations in the dataset. Let $x_i$ be the value of feature $X$ for the $i^{\textrm{th}}$ unit, $1\leq i\leq N$, let $(\overline{x},s_x)$ be the mean and standard deviation of feature $X$, let $\alpha$ be the desired significance level, and let  $T(\alpha,N)$ be the critical value of the Student $t$-distribution at significance $\alpha/2N$. Then, the $i^{\textrm{th}}$ unit is an \textbf{outlier along feature} $X$ if $$|x_i-\overline{x}| \geq \frac{s_x(N-1)}{\sqrt{N}}\sqrt{\frac{T^2(\alpha,N)}{N-2+T^2(\alpha,N)}}.$$
\item Other common tests include:
\begin{itemize}[noitemsep]
\item the \textbf{Dixon $Q$ test}, which is used in the experimental sciences to find outliers in (extremely) small datasets -- it is of dubious validity;
\item the \textbf{Mahalanobis distance}, which is linked to the leverage of an observation (a measure of influence), can also be used to find multi-dimen\-sio\-nal outliers, when all relationships are linear (or nearly linear);
\item the \textbf{Tietjen-Moore} test, which is used to find a specific number of outliers;
\item the \textbf{generalized extreme studentized deviate}, if the number of outliers is unknown; 
\item the \textbf{chi-square} test, when outliers affect the foodneess-of-fit, as well as 
\item DBSCAN and other unsupervised outlier detection methods.
\end{itemize}
\end{itemize}
\subsubsection*{Visual Outlier Detection} The following three (simple) examples illustrate the principles underlying visual outlier and anomaly detection. 
\begin{Example}On a specific day, the height of several plants in a nursery are measured. The records also show each plant's age (the number of days since the seed has been planted). 
\begin{figure*}[t]
\centering
\includegraphics[width=0.32\textwidth]{Images/plant_age}\quad
\includegraphics[width=0.32\textwidth]{Images/plant_height}\quad
\includegraphics[width=0.32\textwidth]{Images/plant_height_vs_age}
\caption[\small Summary visualisations for a plant dataset]{\small Summary visualisations for an (artificial) plant dataset: age distribution (left), height distribution (middle), height vs. age, with linear trend (right).} \label{fig:plant_data}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=0.32\textwidth]{Images/scatter_plot_linear_1}\quad
\includegraphics[width=0.32\textwidth]{Images/scatter_plot_linear_2}\quad
\includegraphics[width=0.32\textwidth]{Images/scatter_plot_2}
\caption[\small Visualisations for a service point dataset]{\small Visualisations for an (artificial) service point dataset: trend for 11 service points (left), trend for 10 service points (middle), influential observations (right).}\hrule
        \label{fig:service_data}
\end{figure*}
\begin{figure*}[t]
\centering\includegraphics[width=0.26\textwidth]{{Images/appendage_length_descriptive}}\quad \includegraphics[width=0.50\textwidth]{{Images/appendage_length}}
\caption[\small Summary and visualisation for an appendage length dataset]{\small Summary and visualisation for an (artificial) appendage length dataset: descriptive statistics (left), appendge length distribution (right).}\hrule
\label{tab:appendage_data}
\end{figure*}

Histograms of the data are shown in Figure~\ref{fig:plant_data} (age on the left, height on the middle). \par Very little can be said about the data at that stage: the age of the plants (controlled by the nursery staff) seems to be somewhat haphazard, as does the response variable (height). A scatter plot of the data (rightmost chart in Figure~\ref{fig:plant_data}), however, reveals that growth is strongly correlated with age during the early days of a plant's life for the observations in the dataset; points clutter around a linear trend. One point (in yellow) is easily identified as an \textbf{outlier}.\par There are at least two possibilities: either that measurement was botched or mis-entered in the database (representing an invalid entry), or that one specimen has experienced unusual growth (outlier). Either way, the analyst has to investigate further.  
  \end{Example}
  \begin{Example}
A government department has 11 service points in a jurisdiction. Service statistics are recorded: the monthly average arrival rates per teller and monthly average service rates per teller for each service point are available. \par A scatter plot of the service rate per teller ($y$ axis) against the arrival rate per teller ($x$ axis), with linear regression trend, is shown in the leftmost chart in Figure~\ref{fig:service_data}. The trend is seen to inch upwards with increasing $x$ values. \par A similar chart, but with the left-most point removed from consideration, is shown in the middle chart of  Figure~\ref{fig:service_data}. The trend still slopes upward, but the fit is significantly improved, suggesting that the removed observation is unduly \textbf{influential} (or anomalous) -- a better understanding of the relationship between arrivals and services is afforded if it is set aside. \par Any attempt to fit that data point into the model must take this information into consideration. Note, however, that influential observations depend on the analysis that is ultimately being conducted -- a point may be influential for one analysis, but not for another.     
\end{Example}
\begin{Example}
Measurements of the length of the appendage of a certain species of insect have been made on 71 individuals. Descriptive statistics have been computed; the results are shown in Figure~\ref{tab:appendage_data}.

\newpage\noindent Analysts who are well-versed in statistical methods might recognize the tell-tale signs that the distribution of appendage lengths is likely to be asymmetrical (since the skewness is non-negligible) and to have a ``fat'' tail (due to the kurtosis being commensurate with the mean and the standard deviation, the range being so much larger than the interquartile range, and the maximum value being so much larger than the third quartile). \par The mode, minimum, and first quartile values belong to individuals without appendages, so there appears to be at least two sub-groups in the population (perhaps split along the lines of juveniles/adults, or males/females). The maximum value has already been seen to be quite large compared to the rest of the observations, which at first suggests that it might belong to an \textbf{outlier}. \par The histogram of the measurements, however, shows that there are 3 individuals with very long appendages (see right-most chart in Figure~\ref{tab:appendage_data}): it now becomes plausible for these anomalous entries to belong to individuals from a different species altogether who were \textbf{erroneously added} to the dataset. This does not, of course, constitute a proof of such an error, but it raises the possibility, which is often the best that an analyst can do in the absence of subject matter expertise.
\end{Example}
\noindent This traditional approach to anomaly detection fails for high-dimensional datasets, however, and a fundamentally different approach is advocated.\newpage  
\subsection{Anomaly Detection as a Statistical Learning Problem}
Fraudulent behaviour is not always easily identifiable, even after the fact. Credit card fraudsters, for isntance, will try to disguise their transactions as regular and banal, rather than as outlandish; to fool human observers into confusing what is mererly \textbf{plausible} with what is \textbf{probable} (or at least, \textbf{not improbable}).\newl At its most basic level, anomaly detection is a problem in \textbf{applied probability}: if $I$ denotes what is known about the dataset (behaviour of individual observations, behaviour of observations as a group, anomalous/normal verdict for a number of similar observations, etc.), is $$P(\text{obs. is anomalous}|I) > P(\text{obs. is normal}|I)?$$ 
Anomaly detection models usually assume \textbf{stationarity for normal observations}, which is to say, that the underlying mechanism that generates data does not change in a substantial manner over time, or, if it does, that its rate of change or cyclicity is known. For time series data, this means that it may be necessary to perform trend and seasonality extraction in order to identify anomalies.
\begin{Example} Supply chains play a crucial role in the transportation of goods from one part of the world to another. As the saying goes, ``a given chain is only as strong as its weakest link'' -- in a multi-modal context, comparing the various transportation segments is far from an obvious endeavour: if shipments departing Shanghai in February 2013 took two more days, on average, to arrive in Vancouver than those departing in July 2017, can it be said with any certainty that the shipping process has improved in the intervening years? Are February departures always slower to cross the Pacific Ocean? \par The seasonal variability of performance is relevant to supply chain monitoring and the ability to quantify and account for the severity of its impact on the data is thus of great interest. \par 
One way to tackle this problem is to produce an \textbf{index} to track container transit times. This index should depict the \textbf{reliability} and the \textbf{variability} of transit times but in such a way as to be able to allow for performance comparison between differing time periods.\newl 
To simplify the discussion, assume that the ultimate goal is to compare quarterly and/or monthly performance data, irrespective of the transit season, in order to determine how well the network is performing on the \textit{Shanghai} $\to$ \textit{Port Metro Vancouver/Prince Rupert} $\to$ \textit{Toronto} corridor, say. 
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Images/mmsc.png}
\caption[\small Visualisations for a service point dataset]{\small Multi-modal supply chain.} \label{fig:mmsc}
\end{figure}
\noindent The supply chain under investigation has Shanghai as the point of origin of shipments, with Toronto as the final destination; the containers enter the country either through Vancouver or Prince Rupert. Containers leave their point of origin by boat, arrive and dwell in either of the two ports before reaching their final destination by rail. \newl 
For each of the three segments (Marine Transit, Port Dwell, Rail Transit), the data consists of the monthly empirical distribution of transit times, built from sub-samples  (assumed to be randomly selected and fully representative) of all containers entering the appropriate segment.
\par
Each segment's performance is measured using \textbf{fluidity indicators}, which are computed using various statistics of the transit/dwelling time distributions for each of the supply chain segments, such as: 
\begin{description}[noitemsep]
\item[Reliability Indicator (RI)] -- the ratio of the 95$^{\text{th}}$ percentile to the 5$^{\text{th}}$ percentile of transit/dwelling times (a high RI indicates high volatility, whereas a low RI $(\approx 1)$ indicates a reliable corridor);
\item[Buffer Index (BI)] -- the ratio of the positive difference between the 95$^{\text{th}}$ percentile and the mean, to the mean. A small BI $(\approx 0)$ indicates only slight variability in the upper (longer) transit/dwelling times; a large BI indicates that the variability of the longer transit/dwelling times is high, and that outliers might be found in that domain;
\item[Coefficient of Variation (CV)] -- the ratio of the standard deviation of transit/dwelling times to the mean transit/dwelling time.  
\end{description}
The time series of monthly indicators (which are derived from the monthly transit/dwelling time distributions in each segment) are then \textbf{decomposed} into their 
\begin{itemize}[noitemsep]
\item trend;
\item seasonal component (seasonality, trading-day, moving-holiday), and 
\item irregular component.
\end{itemize}
The trend and the seasonal components provide the \textbf{expected behaviour} of the indicator time series; the irregular component arose as a consequence of supply chain \textbf{volatility}. A high irregular component at a given time point indicates a poor performance against expectations for that month, which is to say, an \textbf{anomalous observation}.  

\end{Example}
\subsection{Literature Review}
\subsection{Structure and Organization}
\afterpage{\FloatBarrier}